{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for training:\n",
    "#### Step-by-step guide:\n",
    "#### Load the dataset.\n",
    "\n",
    "#### Inspect the data to understand its structure and features.\n",
    "\n",
    "#### Check for missing values and handle them if necessary.\n",
    "\n",
    "#### Encode categorical features if any.\n",
    "\n",
    "#### Normalize or scale numerical features (optional but recommended for logistic regression).\n",
    "\n",
    "#### Split the data into training and testing sets.\n",
    "\n",
    "#### Ready for Logistic Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let me start with loading and inspecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>129.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>916</td>\n",
       "      <td>54</td>\n",
       "      <td>Female</td>\n",
       "      <td>VA Long Beach</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>127.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>True</td>\n",
       "      <td>st-t abnormality</td>\n",
       "      <td>154.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>917</td>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>VA Long Beach</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>False</td>\n",
       "      <td>st-t abnormality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>918</td>\n",
       "      <td>55</td>\n",
       "      <td>Male</td>\n",
       "      <td>VA Long Beach</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>122.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>True</td>\n",
       "      <td>st-t abnormality</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>919</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>VA Long Beach</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>385.0</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>920</td>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>VA Long Beach</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>120.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>93.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>920 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  age     sex        dataset               cp  trestbps   chol    fbs  \\\n",
       "0      1   63    Male      Cleveland   typical angina     145.0  233.0   True   \n",
       "1      2   67    Male      Cleveland     asymptomatic     160.0  286.0  False   \n",
       "2      3   67    Male      Cleveland     asymptomatic     120.0  229.0  False   \n",
       "3      4   37    Male      Cleveland      non-anginal     130.0  250.0  False   \n",
       "4      5   41  Female      Cleveland  atypical angina     130.0  204.0  False   \n",
       "..   ...  ...     ...            ...              ...       ...    ...    ...   \n",
       "915  916   54  Female  VA Long Beach     asymptomatic     127.0  333.0   True   \n",
       "916  917   62    Male  VA Long Beach   typical angina       NaN  139.0  False   \n",
       "917  918   55    Male  VA Long Beach     asymptomatic     122.0  223.0   True   \n",
       "918  919   58    Male  VA Long Beach     asymptomatic       NaN  385.0   True   \n",
       "919  920   62    Male  VA Long Beach  atypical angina     120.0  254.0  False   \n",
       "\n",
       "              restecg  thalch  exang  oldpeak        slope   ca  \\\n",
       "0      lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
       "1      lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
       "2      lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
       "3              normal   187.0  False      3.5  downsloping  0.0   \n",
       "4      lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
       "..                ...     ...    ...      ...          ...  ...   \n",
       "915  st-t abnormality   154.0  False      0.0          NaN  NaN   \n",
       "916  st-t abnormality     NaN    NaN      NaN          NaN  NaN   \n",
       "917  st-t abnormality   100.0  False      0.0          NaN  NaN   \n",
       "918    lv hypertrophy     NaN    NaN      NaN          NaN  NaN   \n",
       "919    lv hypertrophy    93.0   True      0.0          NaN  NaN   \n",
       "\n",
       "                  thal  num  \n",
       "0         fixed defect    0  \n",
       "1               normal    2  \n",
       "2    reversable defect    1  \n",
       "3               normal    0  \n",
       "4               normal    0  \n",
       "..                 ...  ...  \n",
       "915                NaN    1  \n",
       "916                NaN    0  \n",
       "917       fixed defect    2  \n",
       "918                NaN    0  \n",
       "919                NaN    1  \n",
       "\n",
       "[920 rows x 16 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "frame = pd.read_csv('heart_disease_uci.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 920 entries and 16 columns. Here's a breakdown of the columns:\n",
    "\n",
    "id: Identifier for each patient.\n",
    "\n",
    "age: Age of the patient.\n",
    "\n",
    "sex: Gender of the patient (Male/Female).\n",
    "\n",
    "dataset: Source of the dataset (this might not be necessary for prediction).\n",
    "\n",
    "cp: Chest pain type (categorical).\n",
    "\n",
    "trestbps: Resting blood pressure (some missing values).\n",
    "\n",
    "chol: Cholesterol level (some missing values).\n",
    "\n",
    "fbs: Fasting blood sugar (categorical, some missing values).\n",
    "\n",
    "restecg: Resting electrocardiographic results (categorical).\n",
    "\n",
    "thalch: Maximum heart rate achieved (some missing values).\n",
    "\n",
    "exang: Exercise induced angina (categorical, some missing values).\n",
    "\n",
    "oldpeak: ST depression induced by exercise (some missing values).\n",
    "\n",
    "slope: Slope of the peak exercise ST segment (categorical, many missing values).\n",
    "\n",
    "ca: Number of major vessels (0-3) colored by fluoroscopy (many missing values).\n",
    "\n",
    "thal: Thalassemia (categorical, many missing values).\n",
    "\n",
    "num: Diagnosis of heart disease (target variable: 0 = no disease, 1+ = disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps:\n",
    "\n",
    "1.Handle missing values.\n",
    "\n",
    "2.Convert categorical variables into numeric formats.\n",
    "\n",
    "3.Drop irrelevant columns like id and possibly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           0\n",
       "sex           0\n",
       "cp            0\n",
       "trestbps     59\n",
       "chol         30\n",
       "fbs          90\n",
       "restecg       2\n",
       "thalch       55\n",
       "exang        55\n",
       "oldpeak      62\n",
       "slope       309\n",
       "ca          611\n",
       "thal        486\n",
       "num           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop irrelevant columns (id and dataset)\n",
    "df = frame.drop(columns=['id', 'dataset'])\n",
    "\n",
    "# Check the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values:\n",
    "\n",
    "You can fill or drop missing values depending on your strategy:\n",
    "\n",
    "Drop rows with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['trestbps', 'chol', 'thalch', 'oldpeak', 'ca','thal','fbs','restecg','exang','slope'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values with mean/median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numeric columns with the mean= Model Accuracy: 0.----- (or another strategy as needed)\n",
    "# Fill missing values using .loc to avoid view issues\n",
    "df.loc[:, 'trestbps'] = df['trestbps'].fillna(df['trestbps'].mean())\n",
    "df.loc[:, 'chol'] = df['chol'].fillna(df['chol'].mean())\n",
    "df.loc[:, 'thalch'] = df['thalch'].fillna(df['thalch'].mean())\n",
    "df.loc[:, 'oldpeak'] = df['oldpeak'].fillna(df['oldpeak'].mean())\n",
    "df.loc[:, 'ca'] = df['ca'].fillna(df['ca'].mean())\n",
    "\n",
    "\"\"\"# Fill missing values for numeric columns with the median= Model Accuracy: 0.------ (or another strategy as needed)\n",
    "df_cleaned['trestbps'].fillna(df_cleaned['trestbps'].median(), inplace=True)\n",
    "df_cleaned['chol'].fillna(df_cleaned['chol'].median(), inplace=True)\n",
    "df_cleaned['thalch'].fillna(df_cleaned['thalch'].median(), inplace=True)\n",
    "df_cleaned['oldpeak'].fillna(df_cleaned['oldpeak'].median(), inplace=True)\n",
    "df_cleaned['ca'].fillna(df_cleaned['ca'].median(), inplace=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values using .loc\n",
    "df.loc[:, 'slope'] = df['slope'].fillna(df['slope'].mode()[0])\n",
    "df.loc[:, 'thal'] = df['thal'].fillna(df['thal'].mode()[0])\n",
    "df.loc[:, 'restecg'] = df['restecg'].fillna(df['restecg'].mode()[0])\n",
    "df.loc[:, 'exang'] = df['exang'].fillna(df['exang'].mode()[0])\n",
    "df.loc[:, 'fbs'] = df['fbs'].fillna(df['fbs'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Label Encoder for encode categories column to numaric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\1726797587.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cp'] = label_encoder.fit_transform(df['cp'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'cp' is your column with categories\n",
    "label_encoder = LabelEncoder()\n",
    "df['cp'] = label_encoder.fit_transform(df['cp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical variables:\n",
    "\n",
    "Label encoding converts each category into a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\4129581690.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = label_encoder.fit_transform(df[col])\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\4129581690.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = label_encoder.fit_transform(df[col])\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\4129581690.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = label_encoder.fit_transform(df[col])\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\4129581690.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = label_encoder.fit_transform(df[col])\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\4129581690.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = label_encoder.fit_transform(df[col])\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1436\\4129581690.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = label_encoder.fit_transform(df[col])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate IQR for numeric columns only\n",
    "Q1 = df[numeric_columns].quantile(0.25)\n",
    "Q3 = df[numeric_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out outliers from the DataFrame for numeric columns only\n",
    "df = df[~((df[numeric_columns] < (Q1 - 1.5 * IQR)) | (df[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalch      0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "num         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the num column contains values ranging from 0 to 4, we can convert it into two classes:\n",
    "\n",
    "0 for no disease\n",
    "\n",
    "1 for any heart disease (i.e., values 1, 2, 3, or 4).\n",
    "\n",
    "Steps to convert num to a binary variable:\n",
    "\n",
    "Convert num column to binary:\n",
    "\n",
    "If num == 0, it represents no heart disease.\n",
    "\n",
    "If num > 0, it represents the presence of heart disease (convert to 1).\n",
    "\n",
    "Here's the code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n",
      "0    160\n",
      "1    139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert 'num' to a binary variable\n",
    "df.loc[:, 'num'] = df['num'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "\n",
    "# Check the distribution of the binary target variable\n",
    "print(df['num'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['num'])\n",
    "y = df['num']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numerical features (optional for Logistic Regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 3: Save the trained model to a file\n",
    "model_filename = 'logistic_regression_heart_disease.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase two:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.4761 - loss: 1.9603 - val_accuracy: 0.6981 - val_loss: 1.7629 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5124 - loss: 1.7133 - val_accuracy: 0.7736 - val_loss: 1.6162 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5857 - loss: 1.5391 - val_accuracy: 0.7736 - val_loss: 1.4894 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6348 - loss: 1.4318 - val_accuracy: 0.8491 - val_loss: 1.3959 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6092 - loss: 1.3412 - val_accuracy: 0.8113 - val_loss: 1.3052 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7117 - loss: 1.2150 - val_accuracy: 0.8113 - val_loss: 1.2231 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7127 - loss: 1.1698 - val_accuracy: 0.8113 - val_loss: 1.1630 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7198 - loss: 1.0719 - val_accuracy: 0.8302 - val_loss: 1.0991 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7911 - loss: 0.9871 - val_accuracy: 0.8113 - val_loss: 1.0622 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7363 - loss: 1.0359 - val_accuracy: 0.8113 - val_loss: 1.0327 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7476 - loss: 0.9245 - val_accuracy: 0.8302 - val_loss: 0.9872 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7831 - loss: 0.9110 - val_accuracy: 0.8302 - val_loss: 0.9523 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7097 - loss: 0.8924 - val_accuracy: 0.8491 - val_loss: 0.9162 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7587 - loss: 0.9047 - val_accuracy: 0.7547 - val_loss: 0.9240 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7218 - loss: 0.8799 - val_accuracy: 0.8491 - val_loss: 0.8968 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7172 - loss: 0.8009 - val_accuracy: 0.7925 - val_loss: 0.9785 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7043 - loss: 0.7916 - val_accuracy: 0.8302 - val_loss: 1.0518 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7662 - loss: 0.7498 - val_accuracy: 0.8113 - val_loss: 1.0259 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7577 - loss: 0.7413 - val_accuracy: 0.7925 - val_loss: 0.8787 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7296 - loss: 0.8785 - val_accuracy: 0.8491 - val_loss: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7308 - loss: 0.7673 - val_accuracy: 0.8302 - val_loss: 0.8477 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7261 - loss: 0.7679 - val_accuracy: 0.7736 - val_loss: 0.8619 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7645 - loss: 0.7374 - val_accuracy: 0.8113 - val_loss: 0.8427 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7435 - loss: 0.6980 - val_accuracy: 0.8113 - val_loss: 0.9212 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7591 - loss: 0.7011 - val_accuracy: 0.8302 - val_loss: 0.8582 - learning_rate: 0.0010\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8369 - loss: 0.7897 \n",
      "Test Accuracy: 84.91%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Standardize data (DNNs typically benefit from standardizing inputs)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Example with L2 regularization and Dropout\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],), \n",
    "                          kernel_regularizer=regularizers.l2(0.001)),  # L2 regularization\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),  # Dropout to prevent overfitting\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(2, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "  # Reduced learning rate\n",
    "# Compile the model\n",
    "model.compile(optimizer='AdamW', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-4)\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50,  batch_size=8, \n",
    "                    callbacks=[early_stopping,reduce_lr])\n",
    "\n",
    "#print(model.summary())\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Test_Accuracy_84.52%.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Load scaler\\nscaler = joblib.load('scaler.pkl')\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "\"\"\"# Load scaler\n",
    "scaler = joblib.load('scaler.pkl')\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier, Which is ML Algorithim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a Gradient Boosting Classifier (XGBoost)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier, ML Alo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = XGBClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500],\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'max_depth': [8, 5, 4],\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=8, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create  Flask Server with Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the saved Keras model and Scaler\n",
    "model = tf.keras.models.load_model('Test_Accuracy_84.91%.keras')\n",
    "\n",
    "# Load the scaler (ensure it's in the same directory)\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the POST request\n",
    "    data = request.json\n",
    "    features = np.array([[\n",
    "        data['age'], data['sex'], data['cp'], data['trestbps'], data['chol'], \n",
    "        data['fbs'], data['restecg'], data['thalach'], data['exang'], \n",
    "        data['oldpeak'], data['slope'], data['ca'], data['thal']\n",
    "    ]])\n",
    "\n",
    "    # Scale the input data\n",
    "    scaled_features = scaler.transform(features)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(scaled_features)\n",
    "    #predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    # Return the result as JSON\n",
    "    return jsonify({\n",
    "        'predicted_class': int(prediction),\n",
    "        'message': 'You are unlikely to have heart disease.' if prediction == 0 else 'You are likely to have heart disease.'\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Frontend Interacting with the Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function to gather user input\n",
    "def get_user_input():\n",
    "    age = st.number_input('Age', min_value=0, max_value=120, value=30)\n",
    "    sex = st.selectbox('Sex (1 = Male, 0 = Female)', [0, 1])\n",
    "    cp = st.selectbox('Chest Pain Type (0-3)', [0, 1, 2, 3])\n",
    "    trestbps = st.number_input('Resting Blood Pressure', min_value=80, max_value=200, value=120)\n",
    "    chol = st.number_input('Cholesterol Level', min_value=100, max_value=600, value=200)\n",
    "    fbs = st.selectbox('Fasting Blood Sugar > 120 mg/dl (1 = True, 0 = False)', [0, 1])\n",
    "    restecg = st.selectbox('Resting Electrocardiographic Results (0-2)', [0, 1, 2])\n",
    "    thalach = st.number_input('Maximum Heart Rate Achieved', min_value=60, max_value=220, value=150)\n",
    "    exang = st.selectbox('Exercise Induced Angina (1 = Yes, 0 = No)', [0, 1])\n",
    "    oldpeak = st.number_input('ST Depression Induced by Exercise', min_value=0.0, max_value=6.0, value=1.0, step=0.1)\n",
    "    slope = st.selectbox('Slope of Peak Exercise ST Segment (0-2)', [0, 1, 2])\n",
    "    ca = st.number_input('Number of Major Vessels (0-4)', min_value=0, max_value=4, value=0)\n",
    "    thal = st.selectbox('Thalassemia (0 = Normal, 1 = Fixed Defect, 2 = Reversible Defect)', [0, 1, 2])\n",
    "\n",
    "    # Create a dictionary with the user input\n",
    "    user_data = {\n",
    "        'age': age, 'sex': sex, 'cp': cp, 'trestbps': trestbps, 'chol': chol,\n",
    "        'fbs': fbs, 'restecg': restecg, 'thalach': thalach, 'exang': exang,\n",
    "        'oldpeak': oldpeak, 'slope': slope, 'ca': ca, 'thal': thal\n",
    "    }\n",
    "\n",
    "    return user_data\n",
    "\n",
    "# Main function for Streamlit app\n",
    "def main():\n",
    "    st.title(\"Heart Disease Diagnostic Chat\")\n",
    "\n",
    "    # Gather input data from user\n",
    "    user_input = get_user_input()\n",
    "\n",
    "    # When user clicks 'Predict' button, send data to Flask API\n",
    "    if st.button(\"Predict\"):\n",
    "        # Send POST request to Flask API\n",
    "        url = 'http://127.0.0.1:5000/predict'  # Flask API URL\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(user_input), headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            prediction = response.json()\n",
    "            st.write(f\"Prediction: {prediction['message']}\")\n",
    "        else:\n",
    "            st.write(\"Error: Could not connect to Flask API.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
